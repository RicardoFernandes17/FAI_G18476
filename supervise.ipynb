{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2\n",
    "## Introduction\n",
    "This project analyzes employee attrition using a dataset and various data science techniques. \n",
    "\n",
    "The primary objectives are:\n",
    "- To explore the data using EDA techniques.\n",
    "- To preprocess the data for machine learning.\n",
    "- To build a classification model to predict attrition.\n",
    "- To visualize and interpret results.\n",
    "\n",
    "## Dataset\n",
    "The dataset contains information about employees, including demographics, job roles, and factors potentially linked to attrition.\n",
    "\n",
    "File Path: WA_Fn-UseC_-HR-Employee-Attrition.csv\n",
    "Shape: Printed at runtime to verify the dimensions of the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Data Preparation\n",
    "For this project i am using the following libraries:\n",
    "- pandas\n",
    "- Matplotlib\n",
    "- Seaborn \n",
    "- scikit-learn \n",
    "- NumPy\n",
    "- mlxtend\n",
    "- KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ricardofernandes/Library/Python/3.11/lib/python/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "file_path = 'WA_Fn-UseC_-HR-Employee-Attrition.csv'\n",
    "employee_data = pd.read_csv(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Checks\n",
    "We will start by settings pandas to display 1000 rows and 1000 columns so that we can check a few more values\n",
    "\n",
    "Checking the shape of the dataset we can see that we have 35 columns and 1470 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will be set to see most of the infomation of any print that i make\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_columns', 1000); \n",
    "\n",
    "print(\"Data shape: {}\".format(employee_data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then check the information of the dataset\n",
    "As we can see we donÂ´t have null values in any column and also the datatypes on the data frame are either int64 or object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns datatypes and missign values to check whether i will need to remove null values\n",
    "employee_data.info()\n",
    "employee_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start by describing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The T is to present the describe in tabular form\n",
    "employee_data.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will try to remove single value columns.\n",
    "Lets start by showing an histogram of every numeric column and then making pie charts for every non-numeric column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram to check for single value columns to remove\n",
    "employee_data.hist(figsize=(20,20))\n",
    "plt.show()\n",
    "\n",
    "# Pie Chart Distrubtion for non numeric cols\n",
    "for col in employee_data.select_dtypes(include='object').columns:\n",
    "    counts = employee_data[col].value_counts()\n",
    "    plt.figure(figsize=(8, 6), facecolor='white')\n",
    "    plt.pie(counts, labels=counts.index, autopct='%1.1f%%', startangle=90)\n",
    "    plt.title(f\"{col} Distribution\")  # f-string for string formatting\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we can remove the EmployeeCount, StandardHours, Over18 since they are single value columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_data.drop([ 'EmployeeCount', 'StandardHours', 'Over18'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to check one "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 7))\n",
    "sns.countplot(data=employee_data,x='JobRole', hue='Attrition', palette='viridis')\n",
    "plt.title('Count of Attrition in Different Job Roles', fontsize=16)\n",
    "plt.xlabel('Job Role', fontsize=13)\n",
    "plt.ylabel('Count', fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features and target\n",
    "X = employee_data.drop(columns=['Attrition'])\n",
    "y = employee_data['Attrition']\n",
    "\n",
    "# Encoding categorical variables\n",
    "categorical_cols = X.select_dtypes(include=[object]).columns.tolist()\n",
    "\n",
    "X = pd.get_dummies(X, columns=categorical_cols, drop_first=True)\n",
    "X = X.astype(int)\n",
    "#Encode Attrition\n",
    "y = LabelEncoder().fit_transform(y)\n",
    "correlation_matrix = X.corr()\n",
    "plt.figure(figsize=(30, 20))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', cbar=True, square=True, \n",
    "            cbar_kws={\"shrink\": .75}, linewidths=.5)\n",
    "plt.title('Correlation Matrix of Features', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the Job Level and Monthly Income are very correlated i am going to remove the Monthly Income since i can work with a range of 5 levels.\n",
    "We can also check a high correlation in the columns Years at company, Years at current Role, Years since last promotion and years with current manager so we will remove Years at current Role, Years since last promotion and years with current manager.\n",
    "Another high correlation we can find is between "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop(columns=['MonthlyIncome','TotalWorkingYears','YearsInCurrentRole','YearsWithCurrManager'])\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "dt_classifier = DecisionTreeClassifier(random_state=1234)\n",
    "dt_classifier = dt_classifier.fit(X_train, y_train) \n",
    "plt.figure(figsize=(40,20))\n",
    "tree.plot_tree(dt_classifier,\n",
    "               feature_names=X.columns,\n",
    "               class_names=list(map(str, np.unique(y))),  # Convert class names to strings\n",
    "               filled=True, rounded=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = dt_classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Classification Report\n",
    "print(classification_report(y_test,y_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=np.unique(y), yticklabels=np.unique(y))\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "employee_data_feature_names = employee_data.columns.values.tolist()[:4]\n",
    "employee_data_features = employee_data[employee_data_feature_names]\n",
    "\n",
    "employee_data_target = employee_data[employee_data.columns.values.tolist()[4]]\n",
    "employee_data_target_names = list(set(employee_data_target))\n",
    "\n",
    "print('Features:',employee_data_feature_names, '   Classes:', employee_data_target_names)\n",
    "\n",
    "# Instantiate the model\n",
    "cv_classifier = DecisionTreeClassifier(random_state=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model using cross validation\n",
    "acc_score = cross_val_score(cv_classifier, employee_data_features, employee_data_target, cv=10)\n",
    "print(\"CV Mean Accuracy: %0.3f (+/- %0.3f)\" % (acc_score.mean(), acc_score.std()) )\n",
    "\n",
    "f1_score = cross_val_score(cv_classifier, employee_data_features, employee_data_target, cv=10, scoring='f1_macro')\n",
    "print(\"CV Mean F1: %0.3f (+/- %0.3f)\" % (np.mean(f1_score), np.std(f1_score)) )\n",
    "\n",
    "# Build the model with the complete data\n",
    "#final_classifier = cv_classifier.fit(iris_features, iris_target)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
